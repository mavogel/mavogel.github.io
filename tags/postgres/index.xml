<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>postgres on Portfolio</title>
    <link>https://mavogel.github.io/tags/postgres/</link>
    <description>Recent content in postgres on Portfolio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 05 Jan 2019 17:01:16 +0100</lastBuildDate>
    
	<atom:link href="https://mavogel.github.io/tags/postgres/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Things that I learned in week 01</title>
      <link>https://mavogel.github.io/posts/things-that-i-learned-2019-cw-01/</link>
      <pubDate>Sat, 05 Jan 2019 17:01:16 +0100</pubDate>
      
      <guid>https://mavogel.github.io/posts/things-that-i-learned-2019-cw-01/</guid>
      <description>Postgres Problem with lots of partitions Gaining lots of knowledge on Postgres 10 especially about partioning, we observed a strange behavior on UPDATE and DELETE lately while the number of partitions was growing a lot &amp;gt; 1500.
To recap this is the table structure
CREATE TABLE IF NOT EXISTS projection_table ( id varchar(200) NOT NULL, provider_id varchar(60) NOT NULL, input varchar(4096) NOT NULL, label varchar(100), modified_at timestamp NOT NULL ) PARTITION BY LIST (provider_id); Each partition has 2 more indexes which speed up the queries when the right partition was already determined:</description>
    </item>
    
    <item>
      <title>Things that I learned in week 50</title>
      <link>https://mavogel.github.io/posts/things-that-i-learned-2018-cw-50/</link>
      <pubDate>Sun, 16 Dec 2018 22:44:15 +0100</pubDate>
      
      <guid>https://mavogel.github.io/posts/things-that-i-learned-2018-cw-50/</guid>
      <description>Postgres While creating a projection for a specific use-case, we faced the problem that there was no more room for optimization with indexes on the database. So we had to step back and re-think the whole data model which consisted of 4 tables each having more than 50 million rows. Until this point, we hat cheap inserts and expensive gets on the data model.
After the analysis of the data, we figured out that the approach we took, in the beginning, was a bit too naive.</description>
    </item>
    
  </channel>
</rss>